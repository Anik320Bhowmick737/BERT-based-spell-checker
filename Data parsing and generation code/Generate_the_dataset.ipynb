{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1C-tql_H312",
        "outputId": "88828b47-261e-4550-f240-4c57ea8e4eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=c89c0e1e4b4669b66730e3c1352ca087aaa97cac6afbc44a77378c180f0588a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vxamf97VeKxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gpoOIzitlUY",
        "outputId": "3b493a8a-5ee3-4018-a655-cd468b3c31f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.6.2)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api\n",
        "!pip install wikipedia\n",
        "\n",
        "import wikipediaapi\n",
        "import wikipedia\n",
        "import random\n",
        "import wikipediaapi\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "\n",
        "##  useless code\n",
        "\n",
        "\"\"\"# Initialize Wikipedia API\n",
        "wiki = wikipediaapi.Wikipedia('MyProjectName (merlin@example.com)','en')\n",
        "\n",
        "def filter_non_english_parts(sentence):\n",
        "    try:\n",
        "        # Tokenize the sentence into words\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        #print(words)\n",
        "        filtered_words = []\n",
        "\n",
        "        for word in words:\n",
        "          # Check if the word contains only Latin characters\n",
        "          if all(ord(char) < 128 for char in word):\n",
        "            filtered_words.append(word)\n",
        "\n",
        "        # Join the filtered words back into a sentence\n",
        "        filtered_sentence = ' '.join(filtered_words)\n",
        "        filtered_sentence = filtered_sentence.replace(' ,', ',').replace(' .', '.').replace(' ;', ';').replace(' ?', '?').replace(' !', '!').replace('( ', '(').replace(' )', ')')\n",
        "        return filtered_sentence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sentence: {sentence}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to fetch Wikipedia content with a maximum of 10 sentences\n",
        "def fetch_wikipedia_content(topic):\n",
        "    page = wiki.page(topic)\n",
        "    if not page.exists():\n",
        "        return None\n",
        "\n",
        "    # Tokenize content into sentences\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    sentences = tokenizer.tokenize(page.text)\n",
        "\n",
        "    # Limit to maximum 10 sentences\n",
        "    max_sentences = 7\n",
        "    limited_sentences = sentences[:max_sentences]\n",
        "\n",
        "    filtered_sentences = [filter_non_english_parts(sentence) for sentence in limited_sentences]\n",
        "\n",
        "    return ' '.join(filtered_sentences)\n",
        "\n",
        "def fetch_random_wikipedia_topics(num_topics):\n",
        "    topics = set()\n",
        "\n",
        "    # Fetch random Wikipedia articles until we collect the desired number of topics\n",
        "    while len(topics) < num_topics:\n",
        "        random_title = wikipedia.random()\n",
        "        try:\n",
        "            page = wikipedia.page(random_title)\n",
        "            # Check if the page is in English by inspecting the URL\n",
        "            if '/wiki/' in page.url:  # English pages have '/wiki/' in the URL\n",
        "                topics.add(page.title)\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            continue\n",
        "        except wikipedia.exceptions.DisambiguationError:\n",
        "            continue\n",
        "\n",
        "    return list(topics)\"\"\""
      ],
      "metadata": {
        "id": "Vltdr9q8uFFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcee5417-c55b-4ff4-d4de-6db30028af2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.6.2)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"get_topics = fetch_random_wikipedia_topics(30000)\n",
        "dictionary = {}\n",
        "for topics in get_topics:\n",
        "  dictionary[topics] = fetch_wikipedia_content(topics)\"\"\""
      ],
      "metadata": {
        "id": "DlhDdU3Huihs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data = pd.DataFrame(list(dictionary.items()),columns=[\"Title\",\"Final\"])"
      ],
      "metadata": {
        "id": "6tpZ2WnV1ZEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data.to_csv(\"Bert_finetuning_corpus_wiki_2.csv\",index=False)"
      ],
      "metadata": {
        "id": "nWskQLH_xEmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "Data = pd.read_csv(\"/content/ROCStories__spring2016 - ROCStories_spring2016.csv\")\n",
        "Data2 = pd.read_csv(\"/content/ROCStories_winter2017 - ROCStories_winter2017.csv\")"
      ],
      "metadata": {
        "id": "2Oe1wx7xx3Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data['Final'] = Data['sentence1']+' '+Data['sentence2']+' '+Data['sentence3']+' '+Data['sentence4']+' '+Data['sentence5']\n",
        "Data2['Final'] = Data2['sentence1']+' '+Data2['sentence2']+' '+Data2['sentence3']+' '+Data2['sentence4']+' '+Data2['sentence5']"
      ],
      "metadata": {
        "id": "GtDnxkUzbcG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.concat([Data,Data2],axis=0)"
      ],
      "metadata": {
        "id": "jQlNa-6wfmhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.rename(columns={'storytitle': 'Title'}, inplace=True)"
      ],
      "metadata": {
        "id": "MxL1uidOcsXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Final_dataset = new_data[['Title','Final']]"
      ],
      "metadata": {
        "id": "DYhewpFQcAUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Final_dataset.to_csv(\"Bert_finetuning_corpus.csv\",index=False)"
      ],
      "metadata": {
        "id": "fof_x8NDAbiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate list of homonyms"
      ],
      "metadata": {
        "id": "3pAYRCpq2ZAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the webpage containing the homophones list\n",
        "url = \"http://www.singularis.ltd.uk/bifroest/misc/homophones-list.html\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the page\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "tr_elem = soup.find('tr')\n",
        "\n",
        "homonyms = []\n",
        "for word in tr_elem.get_text().replace(',','').split('\\n'):\n",
        "  homo_list = word.split()\n",
        "  if(len(homo_list))>0:\n",
        "    homonyms.append(homo_list)"
      ],
      "metadata": {
        "id": "bMk4brt1Uk_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "max_homonyms = max(len(entry) for entry in homonyms)\n",
        "\n",
        "columns = [f'Homonym_{i+1}' for i in range(max_homonyms)]\n",
        "df = pd.DataFrame(homonyms, columns=columns)\n",
        "\n",
        "df.to_csv(\"List_of_homonyms.csv\",index=False)"
      ],
      "metadata": {
        "id": "fpfNZa0ucVvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dsx7wWxsqnp",
        "outputId": "78e8ed59-f6e4-4678-cecd-2771d7e83f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.10.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.8/349.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.1-py3-none-any.whl (10 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.6.2)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.6.0-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-5.3.0-py3-none-any.whl (6.5 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.8.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.20.0)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=bd40087ffbc3826657f5fe8e9534044d3e3058e6072e05bfd7095e5207f39018\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124737 sha256=084331d09b1e8fb34ca615ba487757d6e34c18785b298f6e538cc7b9b104370c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=b1676cd09257deedd3a2aad9db4f5fce6c2b1ce3076b64fb18ff52b64fe13d0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.10.0 feedparser-6.0.11 jaraco.collections-5.0.1 jaraco.context-5.3.0 jaraco.functools-4.0.1 jaraco.text-3.12.1 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.6.0 zc.lockfile-3.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IxswYB850RL",
        "outputId": "f88c8b68-bd8e-4dfb-fc9b-00527c650577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English story dataset processing"
      ],
      "metadata": {
        "id": "aiFr6GQs2Lcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "homonym_dataframe = pd.read_csv(\"/content/List_of_homonyms.csv\")"
      ],
      "metadata": {
        "id": "oDm1nnmgguXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "def preprocess_text(text):\n",
        "  text = text.replace('`','')\n",
        "  text = text.replace('+','')\n",
        "  text = text.replace('*','')\n",
        "  text = text.replace('/','')\n",
        "  text = text.replace('\"','')\n",
        "  text = text.replace(\";\",',')\n",
        "  text = text.replace(\". .\",\"..\")\n",
        "  text = text.replace('..','.')\n",
        "  text = text.replace(',,',',')\n",
        "  expanded_text = []\n",
        "  for word in text.split():\n",
        "    expanded_text.append(contractions.fix(word))\n",
        "  text = ' '.join(expanded_text)\n",
        "  text = text.replace(\"'s\",\"#\")\n",
        "  text = text.replace(\"s'\",\"~\")\n",
        "  text = text.replace(\"'\",\"\")\n",
        "  text = text.replace(\"#\",\"'s\")\n",
        "  text = text.replace(\"~\",\"s'\")\n",
        "\n",
        "  sentence = []\n",
        "\n",
        "  words = word_tokenize(text)\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "  for word, tag in pos_tags:\n",
        "    sentence.append(word)\n",
        "\n",
        "  return ' '.join(sentence).replace(\" 's\",\"'s\").replace(\" '\",\"'\").replace(\" .\",\".\").replace(\" ,\",\",\").replace(\" ?\",\"?\").replace(\" !\",\"!\")"
      ],
      "metadata": {
        "id": "c-VrVH5mmH7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "Data = pd.read_csv(\"/content/Bert_finetuning_corpus_eng_stories.csv\")\n",
        "\n",
        "Data[\"Final\"] = Data[\"Final\"].progress_map(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXnwXtpItehv",
        "outputId": "dbf79065-5efe-4e21-ce73-f1846055e34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65735/65735 [04:25<00:00, 247.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data[\"Final\"] = Data[\"Final\"].str.replace(\"..\",'.')"
      ],
      "metadata": {
        "id": "5VTKc6NWwr1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.loc[54081,\"Final\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "4mwhSzznzsg2",
        "outputId": "084be502-7e5c-492f-ab78-89550410331a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have a son that I go to the park to play basketball with sometimes. My son observed I kick my leg when shooting a three point shot. He then began to imitate this shot and named it the daddy shot. The following week I went with him to play basketball at the park. Went I got to the park everyone was doing the daddy shot.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data.to_csv(\"Data.csv\",index=False)"
      ],
      "metadata": {
        "id": "fNrYGLdWusAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet, words\n",
        "import contractions\n",
        "from pattern.en import lexeme\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "\n",
        "tags_to_avoid = ['NNP','NNPS','CD','SYM','.',',','?','POS']\n",
        "verb_tags = ['VB','VBZ','VBD','VBG','VBN','VBP']\n",
        "\n",
        "nlt_word_list = words.words()\n",
        "\n",
        "def introduce_realword_error(word):\n",
        "  h_num = len(homonym_dataframe[homonym_dataframe.eq(word).any(axis=1)].dropna(axis=1).values.tolist())\n",
        "  if h_num>=1:\n",
        "    h_list = homonym_dataframe[homonym_dataframe.eq(word).any(axis=1)].dropna(axis=1).values.tolist()[0]\n",
        "    h_list = set(h_list)\n",
        "    h_list.discard(word)\n",
        "    h_list = list(h_list)\n",
        "    return random.choice(h_list)\n",
        "  else:\n",
        "    if random.gauss(0,1)<-1:\n",
        "      return random.choice(nlt_word_list)\n",
        "    return word\n",
        "\n",
        "\n",
        "\n",
        "def generate_errors(sentence):\n",
        "  expanded_text = []\n",
        "  for word in sentence.split():\n",
        "    expanded_text.append(contractions.fix(word))\n",
        "\n",
        "  sentence = ' '.join(expanded_text)\n",
        "  #print(sentence)\n",
        "  words = word_tokenize(sentence)\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  #print(pos_tags)\n",
        "  error_sentence = []\n",
        "  for word, tag in pos_tags:\n",
        "    if tag in tags_to_avoid:\n",
        "      error_sentence.append(word)\n",
        "    else:\n",
        "      if tag in verb_tags:\n",
        "        all_forms = lexeme(word)\n",
        "        #all_forms = [form for form in lexeme(word) if \"'\" not in form]\n",
        "        donot_incl = [\"aren't\", \"wasn't\", \"isn't\", \"don't\", \"didn't\", \"doesn't\", \"weren't\", \"am not\", \"can't\", \"couldn't\", \"hasn't\", \"hadn't\", \"haven't\", \"won't\", \"wouldn't\", \"shouldn't\", \"mustn't\", \"mightn't\", \"mayn't\", \"needn't\", \"ain't\",\"ski'd\",\"daydreams'\"]\n",
        "        final = [verb for verb in all_forms if verb not in donot_incl]\n",
        "        sample = random.choice(final)\n",
        "        error_sentence.append(sample)\n",
        "      else:\n",
        "        if random.random()<0.3:\n",
        "          error_sentence.append(introduce_realword_error(word))\n",
        "        else:\n",
        "          error_sentence.append(word)\n",
        "  return ' '.join(error_sentence).replace(\" 's\",\"'s\").replace(\" '\",\"'\").replace(\" .\",\".\").replace(\" ,\",\",\").replace(\" ?\",\"?\").replace(\" !\",\"!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXfod5KYm0SA",
        "outputId": "83ff634f-4460-4e4a-c819-038ad9c44269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "Data[\"Errors\"] = Data[\"Final\"].progress_map(generate_errors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2hg9VN_hhgW",
        "outputId": "d901db97-b4b8-4702-dbd9-9a831880a2fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65735/65735 [29:24<00:00, 37.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = {\n",
        "    \"I'll\": [\"isle\", \"aisle\"],\n",
        "    \"we'll\": [\"weal\", \"wheel\"],\n",
        "    \"he'll\": [\"heal\", \"heel\"],\n",
        "    \"you'll\": [\"yule\"],\n",
        "    \"aren't\": [\"aunt\"],\n",
        "    \"he'd\": [\"heed\"],\n",
        "    \"it's\": [\"its\"],\n",
        "    \"they're\": [\"their\", \"there\"],\n",
        "    \"who's\": [\"whose\"],\n",
        "    \"we'd\" : [\"weed\"],\n",
        "    \"you're\": [\"yore\",\"yaw\",\"your\"],\n",
        "    \"cannot\": [\"a\"]\n",
        "    }\n",
        "\n",
        "# Replace contractions in Data[\"Errors\"]\n",
        "for original, replacements in contractions.items():\n",
        "    Data[\"Errors\"] = Data[\"Errors\"].str.replace(original, random.choice(replacements))"
      ],
      "metadata": {
        "id": "JJErHxhUsVN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.to_csv(\"Final Training dataset_1.csv\",index=False)"
      ],
      "metadata": {
        "id": "hOTqPGa0saA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas(desc=\"Processing rows\")\n",
        "# generate the labels\n",
        "Data[\"Label\"] = Data[['Final','Errors']].progress_apply(label_generator,axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8FvmM2xShs_",
        "outputId": "e37e198f-b488-433a-8810-3c838428436c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows: 100%|██████████| 65735/65735 [00:18<00:00, 3503.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Wiki texts"
      ],
      "metadata": {
        "id": "dFylL7UQqnd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install contractions pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkz_JiOe6l5m",
        "outputId": "570a170b-f5e6-448e-e467-dd34bd22eb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: pattern in /usr/local/lib/python3.10/dist-packages (3.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.10/dist-packages (from pattern) (1.0.7)\n",
            "Requirement already satisfied: mysqlclient in /usr/local/lib/python3.10/dist-packages (from pattern) (2.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (from pattern) (6.0.11)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from pattern) (20240706)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (from pattern) (1.1.2)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.10/dist-packages (from pattern) (18.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.31.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.0.1)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (3.2.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (3.0.post1)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (5.0.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser->pattern) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.6.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.10/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (4.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.10/dist-packages (from portend>=2.1.1->cherrypy->pattern) (5.6.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.10/dist-packages (from jaraco.collections->cherrypy->pattern) (3.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.3.0)\n",
            "Requirement already satisfied: autocommand in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.2.2)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.10/dist-packages (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern) (1.2.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.8.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.20.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_empty_brackets(text):\n",
        "    return re.sub(r'\\[\\s*\\]|\\(\\s*\\)|\\{\\s*\\}', '', text)\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a test (anik) string with [ ] empty { } brackets.\"\n",
        "cleaned_text = remove_empty_brackets(text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ORuj4qLqnJc",
        "outputId": "0a7a369b-ae46-43ed-b6fa-b1709c5082ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a test (anik) string with  empty  brackets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "from pattern.en import lexeme\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet, words\n",
        "import random\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "\n",
        "def text_preprocess(text):\n",
        "  text = re.sub(r'\\[\\s*\\]|\\(\\s*\\)|\\{\\s*\\}', '', text)\n",
        "  text = text.replace(\"\\\\\",\"\")\n",
        "  text = text.replace(\"[\",\"(\").replace(\"]\",\")\")\n",
        "  text = re.sub(r'\\.{2,}', ' ', text)\n",
        "\n",
        "  expanded_text = []\n",
        "  for word in text.split():\n",
        "    expanded_text.append(contractions.fix(word))\n",
        "\n",
        "  text = ' '.join(expanded_text)\n",
        "  tokens = word_tokenize(text)\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "  sentence = []\n",
        "  for word, tag in pos_tags:\n",
        "    sentence.append(word)\n",
        "  return ' '.join(sentence).replace(\" s'\",\"s'\").replace(\" s \",\" 's \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9TkcisSry6h",
        "outputId": "45ccbad9-4619-4818-c49a-2698bd31f439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wiki texts"
      ],
      "metadata": {
        "id": "LD6swPd-2R6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_1 = pd.read_csv(\"/content/Corpus_first_wiki.csv\")\n",
        "Data_2 = pd.read_csv(\"/content/Corpus_second_wiki.csv\")\n",
        "Data_3 = pd.read_csv(\"/content/Corpus_third_wiki.csv\")\n",
        "#Data_4 = pd.read_csv(\"/content/Bert_finetuning_corpus_wiki.csv\")"
      ],
      "metadata": {
        "id": "x8iAz4pMbqto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data = pd.concat([Data_1,Data_2,Data_3],axis=0)"
      ],
      "metadata": {
        "id": "6PsEcvPUcswp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.reset_index(inplace=True,drop=True)"
      ],
      "metadata": {
        "id": "1kBiqoS1xeBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data['Title'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOV-d775xw0U",
        "outputId": "574de1f2-a5ca-4a1b-83a0-3f76500c856d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Title\n",
              "Alliance Française de Lima                   1\n",
              "Explorer 41                                  1\n",
              "The Peak Galleria                            1\n",
              "History of France's civil nuclear program    1\n",
              "Brain Balance                                1\n",
              "                                            ..\n",
              "Molly White (writer)                         1\n",
              "Robert W. Jackson                            1\n",
              "Cho Doo-soon case                            1\n",
              "Manuela Rottmann                             1\n",
              "Charles Spencer (cricketer)                  1\n",
              "Name: count, Length: 30000, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Processing rows\")\n",
        "Data[\"Final\"] = Data[\"Final\"].progress_map(text_preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59y48n6abLGX",
        "outputId": "21770a34-f050-46b4-8ca1-d0027350cdf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows: 100%|██████████| 30000/30000 [05:41<00:00, 87.94it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_to_avoid = [\".\",\":\",'NNP','NNPS','CD','SYM',',','POS','``','(',')']\n",
        "verb_tags = ['VB','VBZ','VBD','VBG','VBN','VBP']"
      ],
      "metadata": {
        "id": "GJUqFtCRvNbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.to_csv(\"Final_wiki_data.csv\",index=False)"
      ],
      "metadata": {
        "id": "Fka1WA-Kc5tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "nlt_word_list = words.words()\n",
        "\n",
        "homonym_dataframe = pd.read_csv(\"/content/List_of_homonyms.csv\")\n",
        "\n",
        "def introduce_realword_error(word):\n",
        "  h_num = len(homonym_dataframe[homonym_dataframe.eq(word).any(axis=1)].dropna(axis=1).values.tolist())\n",
        "  if h_num>=1:\n",
        "    h_list = homonym_dataframe[homonym_dataframe.eq(word).any(axis=1)].dropna(axis=1).values.tolist()[0]\n",
        "    h_list = set(h_list)\n",
        "    h_list.discard(word)\n",
        "    h_list = list(h_list)\n",
        "    return random.choice(h_list)\n",
        "  else:\n",
        "    if random.gauss(0,1)<-2:\n",
        "      return random.choice(nlt_word_list)\n",
        "    return word\n",
        "\n",
        "\n",
        "\n",
        "def generate_errors(sentence):\n",
        "  expanded_text = []\n",
        "  for word in sentence.split():\n",
        "    expanded_text.append(contractions.fix(word))\n",
        "\n",
        "  sentence = ' '.join(expanded_text)\n",
        "  #print(sentence)\n",
        "  words = word_tokenize(sentence)\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  #print(pos_tags)\n",
        "  error_sentence = []\n",
        "  for word, tag in pos_tags:\n",
        "    if tag in tags_to_avoid:\n",
        "      error_sentence.append(word)\n",
        "    else:\n",
        "      if tag in verb_tags:\n",
        "        #all_forms = lexeme(word)\n",
        "        all_forms = [form for form in lexeme(word) if \"'\" not in form and \"not\" not in form]\n",
        "        donot_incl = [\"aren't\", \"wasn't\", \"isn't\", \"don't\", \"didn't\", \"doesn't\", \"weren't\", \"am not\", \"can't\", \"couldn't\", \"hasn't\", \"hadn't\", \"haven't\", \"won't\", \"wouldn't\", \"shouldn't\", \"mustn't\", \"mightn't\", \"mayn't\", \"needn't\", \"ain't\",\"ski'd\",\"daydreams'\"]\n",
        "        final = [verb for verb in all_forms if verb not in donot_incl]\n",
        "        if len(final)!=0:\n",
        "          sample = random.choice(final)\n",
        "          error_sentence.append(sample)\n",
        "        else:\n",
        "          error_sentence.append(word)\n",
        "      else:\n",
        "        if random.random()<0.3:\n",
        "          error_sentence.append(introduce_realword_error(word))\n",
        "        else:\n",
        "          error_sentence.append(word)\n",
        "  return ' '.join(error_sentence)\n",
        "\n",
        "contraction = {\n",
        "    \"I'll\": [\"isle\", \"aisle\"],\n",
        "    \"we'll\": [\"weal\", \"wheel\"],\n",
        "    \"he'll\": [\"heal\", \"heel\"],\n",
        "    \"you'll\": [\"yule\"],\n",
        "    \"aren't\": [\"aunt\"],\n",
        "    \"he'd\": [\"heed\"],\n",
        "    \"it's\": [\"its\"],\n",
        "    \"they're\": [\"their\", \"there\"],\n",
        "    \"who's\": [\"whose\"],\n",
        "    \"we'd\" : [\"weed\"],\n",
        "    \"you're\": [\"yore\",\"yaw\",\"your\"],\n",
        "    \"cannot\": [\"a\"],\n",
        "    \"we're\":[\"were\",\"weir\"]\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "lj2yEx4u4yJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data[\"Errors\"] = Data[\"Final\"].progress_map(generate_errors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkvnaOnufpTR",
        "outputId": "4e74c7bb-e2ad-4901-bfc2-3e8c6502f06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows: 100%|██████████| 30000/30000 [36:23<00:00, 13.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data[\"Errors\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTVPUaBEpQur",
        "outputId": "f9cc8040-0d90-44c5-a7f0-66e537a848cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        The Alliance de Lima is a non-profit , non gov...\n",
              "1        The Hildesheim Cathedral Museum ( German : Dom...\n",
              "2        San Micheletto is a Baroque- stile , citadel R...\n",
              "3        Griffith Davies ( 5 December 1788 - 25 March 1...\n",
              "4        Quichotte ( UK : kee-SHOT , French : ) is a 20...\n",
              "                               ...                        \n",
              "29995    San Nicola a Nilo was a Baroque-style Roman Ca...\n",
              "29996    Morpheus being a fictional character in The Ma...\n",
              "29997    Jan Sandee ( 9 December 1919 in Rotterdam 21 J...\n",
              "29998    Jean-Baptiste de La Chapelle ( , Paris ) is a ...\n",
              "29999    Charles Richard Spencer ( 21 June 1903 29 Sept...\n",
              "Name: Errors, Length: 30000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace contractions in Data[\"Errors\"]\n",
        "for original, replacements in contraction.items():\n",
        "    Data[\"Errors\"] = Data[\"Errors\"].str.replace(original, random.choice(replacements))"
      ],
      "metadata": {
        "id": "zd4j0dUFfgzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_generator(txt):\n",
        "    c = word_tokenize(txt[0])\n",
        "    i = word_tokenize(txt[1])\n",
        "    label = []\n",
        "    for w1,w2 in zip(c,i):\n",
        "        if w1!=w2:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "\n",
        "    return label"
      ],
      "metadata": {
        "id": "omGMUsz0pfGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data[\"Label\"] = Data[['Final','Errors']].progress_apply(label_generator,axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLlewK2rpzsP",
        "outputId": "054a9023-1bae-4da2-d842-3c4e8490994b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows: 100%|██████████| 30000/30000 [01:25<00:00, 352.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data[\"Label\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLlYTmQ6p3Cc",
        "outputId": "ca076d6a-e94a-4b89-e457-6cf5b045c292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...\n",
              "2        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...\n",
              "3        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\n",
              "4        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
              "                               ...                        \n",
              "29995    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "29996    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...\n",
              "29997    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, ...\n",
              "29998    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
              "29999    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "Name: Label, Length: 30000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If there are more than 3 consecutive errors remove those samples as they will make the whole sentence lose the context\n",
        "consec_ones = []\n",
        "for i,items in enumerate(Data[\"Label\"].values):\n",
        "    max_len = 0\n",
        "    counter = 0\n",
        "    for j in items:\n",
        "        if j==1:\n",
        "            counter+=1\n",
        "        else:\n",
        "            max_len = max(max_len,counter)\n",
        "            counter=0\n",
        "    max_len = max(max_len,counter)\n",
        "    consec_ones.append(max_len)\n",
        "\n",
        "import numpy as np\n",
        "arr = np.array(consec_ones)\n",
        "indices = np.where(arr > 3)[0]\n",
        "print(len(indices))\n",
        "#print(indices)\n",
        "#Data.drop(indices,inplace = True)\n",
        "#Data.reset_index(inplace = True,drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81b3v7alqBOC",
        "outputId": "03f8f545-8718-44d7-88b3-b1a1ba2a4046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data.to_csv(\"wiki_dataset_with_errors.csv\",index=False)"
      ],
      "metadata": {
        "id": "FxOFMpg2wpfH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}